\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[margin=1in,headheight=13.6pt]{geometry}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{thrm}
\newtheorem{thrm}{Theorem}[section]
\theoremstyle{lma}
\newtheorem{lma}{Lemma}[section]
\theoremstyle{ppst}
\newtheorem{ppst}{Proposition}[section]
\theoremstyle{crlr}
\newtheorem{crlr}{Corollary}[section]
\usepackage{graphicx}
\renewcommand{\baselinestretch}{1.5}
\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}
\usepackage{color}  
\usepackage{hyperref}
\hypersetup{
    colorlinks=true
    linktoc=all
    linkcolor=blue
}
\usepackage{fancyheadings}
\pagestyle{fancyplain}
\fancypagestyle{plain}{
\renewcommand{\headrulewidth}{0.4pt}
}
\lhead{\fancyplain{Haoyue(Heather) Tan}{Haoyue(Heather) Tan}}
\rhead{\fancyplain{STA303 lecture Notes}{STA303 Lecture Notes}}
\title{STA303 - Methods of Data Analysis II}
\author{Heather Tan}
\date{Jan - Apr 2020}
\begin{document}

\maketitle	
\tableofcontents
\pagebreak

\section{Linear model recap}
\textbf{Why model?}
The goal of a model is to provide a (relative) simple summary of a data set. We can use it to describe data and make predictions.
\subsection{The general Linear Model}
\begin{align*}
	Y = \beta_0+\beta_1x_{1i}+\cdots+\beta_p x_{pi}+\epsilon_i\\
	DATA = MODEL+ ERROR
\end{align*}
The model is \textbf{general }since it can have multiple response variables. It is \textbf{linear }because the model is linear in parameters.\\
E.g.
\begin{align*}
	y_i = \beta_0+\gamma_1\delta_1x_{1i}+exp(\beta_2)x_{2i}+\epsilon_i =  \beta_0+\gamma_1\delta_1x_{1i}+e^{\beta_2}x_{2i}+\epsilon_i
\end{align*} is \textbf{linear} since it can be replace to  $y_i = \beta_0+c_1 x_{1i}+c_2x_{2i}+\epsilon_i$
\begin{align*}
	y_i &= \beta_0+\beta_1x_i^{\beta_2}+\epsilon_i\\
	y_i &= \beta_0exp(\beta_1x_{1i})+\epsilon_1
\end{align*}
is \textbf{not linear} since it can be replaced as $y_i = c_0+c_1x_i^{c_2}+\epsilon$ or $y_i = c_0\cdot e^{c_1x_1}+\epsilon$

\subsection{Linear regression assumptions}
\begin{enumerate}
	\item Errors are independent
	\item Errors are identically distributed with $E[\epsilon_i]=0$
	\item Homoscedasticity $Var[\epsilon_i] = \sigma^2$
	\item \textbf{A straight-line relationship exists between $\epsilon_i$ and $y_i$}
\end{enumerate}
$1-3 \implies \epsilon_i \sim N(0,\sigma^2)$


\section{ANOVA}
Analysis of Variance is a statistical method used to test differences between two or more means. It is useful since in real life we often want to compare more than two groups\\
Two-sample t-tests can be problematic:
\begin{itemize}
	\item increasing the risk of Type I error$(H_1|H_0 True)$
	\item At $\alpha = 0.05$, with 100 comparison, 5 will show a difference when none exists
\end{itemize}

\subsection{ANOVA Hypothesis}
$H_0: \mu_1 = \cdots = \mu_n\\$
$H_1$: at least one mean is different from others

\subsection{ANOVA assumptions}
$\epsilon_i \sim N(0,\sigma^2)$

\subsubsection{A not on Normality}
If N is large, Central limit Theorem can be used to relax the normality assumption.
The normality assumption is most important when
\begin{itemize}
	\item n is small
	\item highly non-normal
	\item small effective size
\end{itemize}

\subsection{ANOVA test}
\begin{itemize}
	\item \textbf{Group Mean: }$\hat{\mu_j} = mean\{\hat{y_j}, j = 1,\cdots,J_i\}$
	\item \textbf{Grand Mean: }$\hat{\mu_0} = mean\{\hat{y_{ij}},i = 1,\cdots,k j = 1,\cdots,J_i\}$
	\item want to test:\textbf{Are the $\hat{\mu_i}$ close to $\hat{\mu_0}$}?
\end{itemize}
\begin{align*}
	\frac{\sum_j n_j(\hat{\mu}_j-\hat{\mu}_0)^2/(k-1)}{\sum_{ij}(\hat{y}_{ij}-\hat{\mu}_i)^2/(N-k)}&\sim F_{N-1, M-1}\\
	\frac{\text{variability between groups}}{\text{variability within groups}} &\sim \text{F-distribution}_{obs-1, obs-groups}\\
	\frac{s_b^2}{s_w^2} &\sum F_{N-1,N-k}
\end{align*}
\subsubsection{Last commands on Anova}
\begin{itemize}
	\item ANOVA is testing, not estimating but it uses a linear model structure to get the variation between and within groups
	\item ANOVA tells us if at least on of the group means is different but doesn't give us insight into how the group means are different
	\item ANOVA will reject $H_0$ for large dataset
	\item \textbf{ANOVA can be useful when a dataset is small, for large data set, fit a random effects model.}
\end{itemize}

\section{Designing matrics}
Whenever you have categorical variables in your dataset, R is making you dummy variables for the levels of those categorical variables. Basically a true or false for if each observation take the given level of the categorical variable


\section{Likelihood ratio(LR) test}
The likelihood-ratio test lets us compare the goodness of fit of two competing models based on the ratio of their likelihoods. \\
\textbf{Additional info link:} \url{https://www.zhihu.com/question/51287367/answer/149207450}

\subsection{Maximum Likelihood Estimation}
\begin{itemize}
	\item Model Parameters: $\beta, \sigma$
	\item Likelihood function: $L(\beta,\sigma|Y) = \pi(Y;\beta,\sigma)$
	\item MLE's: $(\hat{\beta}, \hat{sigma}) = \stackrel{argmax}{\beta,\sigma}L(\beta,\sigma|Y)$
\end{itemize}

\subsection{LR Hypothesis}
\begin{itemize}
	\item $H_0:Y_{ij}\sim N(\mu_0, \sigma^2)$ i.e errors are iid noise
	\item $H_1:Y_{ij}\sim N(\mu_j, \sigma^2)$ i.e erros vary by explanatory variable
	\item Very low chance of observing a LR(likelihood ratio) too big if $H_0$ were true
\end{itemize}

\subsection{LR test interpreting}
\begin{itemize}
	\item likelihood under $H_1$ is always higher than under $H_0$
	\begin{itemize}
		\item having different $\hat{\mu}_i$ is possibly to have higher likelihood than all same $\hat{\mu}$
		\item if $H_0$ true, the alternative hypothesis likelihood shouldn't be much larger
	\end{itemize}
	\item 2 times the difference in log likelihood will follow a chi-square distribution if $H_0$ true.
\end{itemize}


\section{Generalized linear Models}
Generalized linear models are a flexible class of models that let us generalise from the linear model to include more types of response variables, such like count, binary and proportion data.

\subsection{Assumptions of the GLM}
\begin{itemize}
	\item GLM does not assume a linear relationship between the dependent variable and the independent variables, but it does assume a linear relationship between the transformed response(in terms of ink function) and the explanatory variables. 
	\item \textbf{Dependent variable:}
	\begin{itemize}
		\item Independently distributed
		\item Can have a distribution from any of the exponential family: binomial, poisson, multinomial or normal
		\item Dependent variable is not assumed to have a linear relationship between it and independent variable. i.e LHS can have non linear transformation
	\end{itemize}
	\item \textbf{Error: }errors are independent but not necessarily normal distributed 
	\item \textbf{Independent variable: }
	\begin{itemize}
		\item Have a linear relationship between the transformed response and independent variables i.e RHS must be a linear formula
		\item Can be the power terms or some other none linear transformations of the original independent variables.
	\end{itemize}
	\item The homogeneity of variance does not need to be satisfied
	\item It use MLE in stead of OLS to estimate the parameters, thus relies on large-sample approximations.
\end{itemize}
\subsection{Components of GLM}
\begin{enumerate}
	\item \textbf{random component: }the response and an associated probability distribution. i.e y and its distribution
	\item \textbf{systematic component: }explanatory variables and relationships among them. i.e x and $\beta$
	\item \textbf{link function: }which tell us about the relationship between the systematic component(or linear predictor) The link function allows us to generalize the linear models for count, binomial and percent data. i.e the link between x and y 
\end{enumerate}

\subsection{Model of GLM}
\begin{align*}
	Y_i &\sim G(\mu_i, \theta)\\
	h(\mu_i) &= X_i^T\beta
\end{align*}
\begin{itemize}
	\item G is the distribution of the response variable
	\item $\mu_i$ is a location parameter for the obseration i
	\item $\theta$ are the additional parameters for the density of G
	\item h is a link function
	\item $X_i$ are covariates for observation i
	\item $\beta$ is a vector of regression coefficients
\end{itemize}

\subsubsection{OLS comparison}
OLS:
\begin{align*}
	Y_i &\sim N(\mu_i, \sigma^2)\\
	\mu_i &= X_i^T\beta
\end{align*}
 OLS is just a flavor of GLM when 
 \begin{itemize}
 	\item G is a normal distribution
 	\item $\theta$ is the variance parameter, denoted as $\sigma^2$
 	\item h is the density function
 \end{itemize}

\subsection{Binomial (or logistic) regression}
\begin{align*}
	Y_i &\sim Bin(N_i,\mu_i)\\
	log(\frac{\mu_i}{1-\mu_i})&=X_i\beta
\end{align*}
\begin{itemize}
	\item g is a Binomial Distribution or Bernoulli if $N_i=1$
	\item h is the logit link. What's logit?: \url{https://zhuanlan.zhihu.com/p/27188729}
	\item $X_i^T\beta$ can be negative
	\item $\mu_i$ is between 0 and 1
\end{itemize}

\subsubsection{Inference: Paramter estimation}
\begin{align*}
	Y_i &\sim N(\mu_i, \sigma^2)\\
	h(\mu_i) &= X_i^T\beta \\
	\pi(Y_1,\cdots,Y_n;\beta,\theta)&= \Pi^N_{i=1} f_G(Y_i;\mu_i,\theta)\\
	log L(\beta,\theta;y_1,\cdots,y_N) &= \sum_{i=1}^N log f_G(y_i;\mu_i,\theta)
\end{align*}
\begin{itemize}
	\item The $Y_i$ are independetnly distributed
	\item \textbf{Joint density} $\pi$ of random variables $(Y_1,\cdots,Y_n)$ is the product of the marginal densities $f_G$
	\item \textbf{Likelihood function} L given observed data $y_1,\cdots,y_N$ is a function of the parameters. 
	\item \textbf{MLE: } $\hat{\beta},\hat{\theta} = argmax_{\beta,\theta}L(\beta,\theta;y_1,\cdots,y_N)$
\end{itemize}
\begin{align*}
	\frac{\partial }{\partial \beta_p}log L(\beta,\theta;y_1,\cdots,y_N) = \sum_{i=1}^N[\frac{\partial }{\partial \mu} log f_G(Y_i;\mu,\theta)]_{\mu = h^{-1}(X_i^T\beta)}[\frac{\partial }{\partial \eta} h^{-1}(\eta)]_{\eta = X_i^T\beta}\cdot X_{ip}
\end{align*}
\begin{itemize}
	\item Information Matrix $I(\hat{\beta}|Y) = \frac{\partial}{\partial \beta\partial \beta^T}-log L(\beta|Y)|\hat{\beta}$
	\item MLEs are approximately Normal $\hat{\beta} \sim MVN(\beta,I(\hat{\beta}|Y)^{-1})$
	\item Standard errors are the root of diagonals of inverted information matrix.
\end{itemize}
\subsubsection{Likelihood ratio tests}
\begin{align*}
	2[logL(\hat{\beta};y)-log L(\beta;y)] \sim \chi_P^2
\end{align*}
where P is the number of parameters in $\beta$\\

\subsubsection{Comparing Nested Models}
Model A is nested in Model B if the parameters in Model A are a subset of the parameters in Model B.
\begin{itemize}
	\item $H_0:\beta_k = C_k \forall k\in\Omega, \quad \Omega \subset \{1,\cdots,P\}$
	\item $H_1:\beta$ unconstrained
	\item \textbf{Nested: }H0 is a special case of H1
	\item Write $\hat{\beta}^{(C)}$ as the constrained MLES under H0
\end{itemize}
\begin{align*}
	2[logL(\hat{\beta};y)-log L(\hat{\beta}^{(C)};y)] \sim \chi_{|\Omega|}^2
\end{align*}

\subsubsection{Interpreting Logistic Models}
\begin{align*}
	Y_i &\sim Bin(N_i, \mu_i)\\
	log(\frac{\mu_i}{1-\mu_i}) &= \sum_{p=1}^PX_{ip}\beta_p\\
	(\frac{\mu_i}{1-\mu_i}) &= \Pi_{p=1}^P exp(\beta_p)^{X_{ip}}
\end{align*}
\begin{itemize}
	\item $\mu_i$ is a probability
	\item $log(\frac{\mu_i}{1-\mu_i})$ is a log-odds
	\item $(\frac{\mu_i}{1-\mu_i})$ is an odds
	\item $\mu \approx 0 \implies \mu_i \approx \mu_i/(1-\mu_i)$
\end{itemize}
Suppose $X_{1p} = X_{2p} \forall p$ except $X_{2q} = X_{1q}+1$
\begin{align*}
	\beta_p &= log(\frac{\mu_2}{1-\mu_2})-log(\frac{\mu_1}{1-\mu_1})\\
	exp(\beta_p) &= (\frac{\mu_2}{1-\mu_2})/(\frac{\mu_1}{1-\mu_1})  
\end{align*}
\begin{itemize}
	\item $\beta_p$ is the log-odds ratio
	\item $exp(\beta_p)$ is the odds ratio
	\item $exp(intercept)$ is baseline odds, when $X_{i2}\cdots X_{ip}=0$
\end{itemize}

\section{w4 - Linear Mixed Models(LMMs)}
\textbf{why we cannot assume independence?: }multiple responses from the same subject cannot be regarded as independent from each other.

\subsection{Model set up}
\begin{align*}
	Y_{ij}|U_i &\sim N(\mu_{ij},\tau^2)\\
	\mu_{ij} &= x_{ij}\beta+U_i\\
	[U_1,\cdots,U_M]^T&\sim MVN(0,\Sigma)
\end{align*}
\begin{itemize}
	\item Observations $Y_{ij}$ for repeated measures j on individuals i
	\item fixed effects $X_{ij}\beta$
	\item random effects $U_i$ for i in 1 to M(new parts that makes this a linear mixed model)
\end{itemize}

\subsection{Assumption of LMMs}
\begin{enumerate}
	\item There is a continuous response variable
	\item we have modeled the dependency structure correctly
	\item Our units/subjects are independent, even though observations within each subjects are taken not to be
	\item Both the random effects and within-unit residual errors follow normal distribution
	\item The random effects errors and within-unit residual errors have constant variance
\end{enumerate}
\subsection{Alternative formulations}
\begin{align*}
	Y_{ij} = X_{ij}\beta +\epsilon_{ij}\\
	\epsilon_{ij} = U_i+Z_{ij}\quad Z_{ij}\sim N(0,\tau^2)\\
	\textbf{OR}\\
	Y_i = X_i\beta+Z_ib_i+\epsilon_i
\end{align*}
\begin{itemize}
	\item $Y_i$ is a vector of outcomes for subject i
	\item $X_i$ and $Z_i$ are model matrices for the fixed and random effects 
	\item The vector $\beta$ describes the effect of covariates on the mean/expectation of the outcome and the vector $b_i$ is the random effects for units
	\item $\epsilon_i $ is the vector of residual errors, normally distributed with a given variance and the errors within units are mutually independent.
\end{itemize}





\section{w5-Generalised Linear Mixed Models(GLMMs)}
\textbf{Pros:}
\begin{itemize}
	\item powerful class of methods that conbined the characteristics of generalized linear models and linear mixed models
	\item Can be used with a range of response distributions(Poi, Bi, Gam)
	\item Can be used in a range of situation where observations are grouped in some way
	\item fas and can be extended to handle somewhere more complex situations
\end{itemize}
\textbf{Cons:}
\begin{itemize}
	\item Some of the standard ways we've learned to test models don't apply
	\item Greater risk of making sensible models that are too complex for our data to support
\end{itemize}

\subsection{Assumption}
\begin{enumerate}
	\item X are independent, even though Y within each subject are taken not to be
	\item Random effects come from a normal distribution
	\item the random effects errors and within-unit residual errors have constant variance 
	\item The chosen link function is appropriate/ the model is correctly specified 
\end{enumerate}



\pagebreak
\section{In-class Questions}
\subsection{Week 5}
Q: The shipd dataset found in the MASS package gives the number of damage incidents and aggregate months of service for different types of ships broken down by year of construction and period of operation. Which model do you think you should fit?\\
A: Generalised linear model(GLM) - Poisson\\
\\
Q: How might we model the risk of mortality for people of different income and education level?\\
A: GLM-Binomial\\
\\
Q: how might we model the number of species on an different island in the Galapagos based on features of each island?\\
A: (Ch3 faraway,week 3 page) GLM- Poisson \\
\\
Q: How might we model the heights of children based on their age, birth weight and mother's age at birth\\
A: Linear model\\
\\
Q: How might we model the achievement scores of students in a standardise test across 15 different hs in TDSB?\\
A: Linear mixed model\\
\\
Q: How might we model the waiting time for ride share cars, based on suburb/neibourhood of the request and time of the day\\
A: GLM-Gamma\\



\section{Notes of Coding}
\begin{itemize}
	\item \textbf{::} - helps to access the exact function from that specific package
	\item R code "myLik" calculates $-logL(\beta,\sigma|Y)$, "optim" minimizes
\end{itemize}

\end{document}
